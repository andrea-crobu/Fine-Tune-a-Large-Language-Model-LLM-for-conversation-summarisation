{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HCIg39APoFk"
   },
   "source": [
    "# Phase 3: Model Evaluation\n",
    "\n",
    "In the final phase, we will evaluate the performance of the fine-tuned Flan-T5 model. The evaluation involves comparing the summarization outputs of the fine-tuned model against the non-fine-tuned (base) model and the ground truth summaries. We will use ROUGE scores and qualitative assessments.\n",
    "\n",
    "## Steps:\n",
    "\n",
    "1. **Compute Evaluation Metrics:**\n",
    "    - **ROUGE Scores:** Measure ROUGE-1, ROUGE-2, and ROUGE-L to assess the quality of the generated summaries.\n",
    "\n",
    "2. **Generate and Compare Summaries:**\n",
    "    - **Qualitative Comparison:** Review summaries generated by the fine-tuned model, the base model, and compare them to the ground truth. Assess the quality and relevance of the generated summaries through manual inspection.\n",
    "    - Identify strengths and weaknesses to guide further improvements.\n",
    "\n",
    "This final phase validates the effectiveness of the fine-tuning process and highlights the improvements achieved in the model's summarization capabilities.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8569,
     "status": "ok",
     "timestamp": 1739714277534,
     "user": {
      "displayName": "Andrea Crobu",
      "userId": "01110819918100512364"
     },
     "user_tz": -60
    },
    "id": "CCo1ZsvEPsYh"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1739714286038,
     "user": {
      "displayName": "Andrea Crobu",
      "userId": "01110819918100512364"
     },
     "user_tz": -60
    },
    "id": "cIBncdt5P2HQ",
    "outputId": "cda98280-3f86-40e3-dbd2-4789d2492222"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R4-x8NejPqpj"
   },
   "source": [
    "### Load the original model and the fine-tuned model (with LoRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8722,
     "status": "ok",
     "timestamp": 1739714297085,
     "user": {
      "displayName": "Andrea Crobu",
      "userId": "01110819918100512364"
     },
     "user_tz": -60
    },
    "id": "utmmOGtqP-kY",
    "outputId": "cfbc18b0-da68-4fa7-d900-5eca9a1e0080"
   },
   "outputs": [],
   "source": [
    "model_path=\"./peft_model_trained_google_flan_t5_base_dialogue_summarization \"\n",
    "\n",
    "model_name='google/flan-t5-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(base_model, model_path, is_trainable=False)\n",
    "peft_model = peft_model.to(device)\n",
    "peft_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3150,
     "status": "ok",
     "timestamp": 1739714300233,
     "user": {
      "displayName": "Andrea Crobu",
      "userId": "01110819918100512364"
     },
     "user_tz": -60
    },
    "id": "FR7yIxBYXSgh",
    "outputId": "dadcd8a9-637f-4d07-f090-eef562205e78"
   },
   "outputs": [],
   "source": [
    "# reload the base model\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "base_model = base_model.to(device)\n",
    "base_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aelzL-ZpR1oZ"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z8Xuru2IR39e"
   },
   "source": [
    "### Run the originl model and the fine-tuned model on the test set and save the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 867,
     "status": "ok",
     "timestamp": 1739714304920,
     "user": {
      "displayName": "Andrea Crobu",
      "userId": "01110819918100512364"
     },
     "user_tz": -60
    },
    "id": "Ybc1a86GP-h6"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the dataset\n",
    "with open('data/dataset_t5_base.pkl', 'rb') as file:\n",
    "    dataset = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PoYXwMDEX57r"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "model_predictions = pd.DataFrame([],columns=['ground_truth', 'base_model', 'fine_tuned_model'])\n",
    "\n",
    "# iterate ove the test set\n",
    "for i in tqdm(range(len(dataset['test']))):\n",
    "  input_ids = dataset['test']['input_ids'][i]\n",
    "  labels = dataset['test']['labels'][i]\n",
    "\n",
    "  # save the ground truth summary\n",
    "  model_predictions.loc[i,'ground_truth'] = tokenizer.decode(labels, skip_special_tokens=True)\n",
    "\n",
    "  # use the base model to predict the summary\n",
    "  outputs_base_model = base_model.generate(\n",
    "        input_ids = torch.tensor(input_ids).unsqueeze(0).to(device),\n",
    "        generation_config=GenerationConfig(max_new_tokens=200)\n",
    "  )\n",
    "\n",
    "  # decode the output and put it in the dataframe\n",
    "  model_predictions.loc[i,'base_model'] = tokenizer.decode(outputs_base_model[0], skip_special_tokens=True)\n",
    "\n",
    "  # use the fine tuned model to predict the summary\n",
    "  outputs_peft_model = peft_model.generate(\n",
    "        input_ids = torch.tensor(input_ids).unsqueeze(0).to(device),\n",
    "        generation_config=GenerationConfig(max_new_tokens=200)\n",
    "  )\n",
    "\n",
    "  # decode the output and put it in the dataframe\n",
    "  model_predictions.loc[i,'fine_tuned_model'] = tokenizer.decode(outputs_peft_model[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "byKx0Kv02xWX"
   },
   "outputs": [],
   "source": [
    "# save the predictions on the Test Set\n",
    "model_predictions.to_csv('data/model_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EpKSY0IvRT28"
   },
   "source": [
    "### Compute the metric ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 518,
     "status": "ok",
     "timestamp": 1739714310506,
     "user": {
      "displayName": "Andrea Crobu",
      "userId": "01110819918100512364"
     },
     "user_tz": -60
    },
    "id": "dVae0RI5CGcD"
   },
   "outputs": [],
   "source": [
    "# load the predictions of the test set\n",
    "model_predictions = pd.read_csv('data/model_predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 269,
     "status": "ok",
     "timestamp": 1739714332168,
     "user": {
      "displayName": "Andrea Crobu",
      "userId": "01110819918100512364"
     },
     "user_tz": -60
    },
    "id": "M4QmOvhbB82s"
   },
   "outputs": [],
   "source": [
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "7fb6555be25f4d248696492d94a76991",
      "de3dd22c64574a28913add706bcf43ff",
      "cfda7a20ca3f44b9abb2f69072e65b2c",
      "0d5e775f30a4499ca6c1a33d49e590c2",
      "52d7182a3c194476b90fa7f0468ea1cc",
      "d4ac0f045e174362b1e5822bf10bddb3",
      "7a624b5d438c4168bc3559934875e996",
      "44669087829243e19c428157abc7dd58",
      "cdf842a7fecd4be3953acf4321735144",
      "cd689a2f359e4e8095bd7eb0d77d0291",
      "27dcd16290fc451fbfa469b0a1057227"
     ]
    },
    "executionInfo": {
     "elapsed": 5034,
     "status": "ok",
     "timestamp": 1739714345212,
     "user": {
      "displayName": "Andrea Crobu",
      "userId": "01110819918100512364"
     },
     "user_tz": -60
    },
    "id": "rFNKjXwrRXu9",
    "outputId": "d1266a71-e3ea-4dd3-c3ee-81b48c6c61b9"
   },
   "outputs": [],
   "source": [
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9538,
     "status": "ok",
     "timestamp": 1739714624550,
     "user": {
      "displayName": "Andrea Crobu",
      "userId": "01110819918100512364"
     },
     "user_tz": -60
    },
    "id": "T_0rnmPBP-fW"
   },
   "outputs": [],
   "source": [
    "# compute the score for the base model and for the fine tuned model\n",
    "\n",
    "base_model_results = rouge.compute(\n",
    "    predictions=model_predictions['base_model'].to_list(),\n",
    "    references=model_predictions['ground_truth'].to_list(),\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "peft_model_results = rouge.compute(\n",
    "    predictions=model_predictions['fine_tuned_model'].to_list(),\n",
    "    references=model_predictions['ground_truth'].to_list(),\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 546,
     "status": "ok",
     "timestamp": 1739714694322,
     "user": {
      "displayName": "Andrea Crobu",
      "userId": "01110819918100512364"
     },
     "user_tz": -60
    },
    "id": "Fong2eiCEd8K",
    "outputId": "5870cb72-c36c-41ed-ca49-7cb29ef628ab"
   },
   "outputs": [],
   "source": [
    "print('Base Model scores:')\n",
    "for score in base_model_results:\n",
    "  print(f'{score}: {base_model_results[score]}')\n",
    "\n",
    "print('\\n --------------- \\n')\n",
    "\n",
    "print('Fine-tuned Model scores:')\n",
    "for score in peft_model_results:\n",
    "  print(f'{score}: {peft_model_results[score]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MkAo2SbPFaZO"
   },
   "source": [
    "#### **We can notic how all the rouge scores improved with fine-tuning!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tg6yIMF1Psk3"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 577,
     "status": "ok",
     "timestamp": 1739715213856,
     "user": {
      "displayName": "Andrea Crobu",
      "userId": "01110819918100512364"
     },
     "user_tz": -60
    },
    "id": "dbiwqNZ_PoFm"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# function to format the dialogue and remove the intro and summary prompt\n",
    "def format_dialogue(text):\n",
    "    # Remove the intro and summary prompt\n",
    "    text = re.sub(r\"^Here is a dialogue:\\s*\", \"\", text)  # Remove starting phrase\n",
    "    text = re.sub(r\"\\s*Write a short summary!$\", \"\", text)  # Remove ending phrase\n",
    "\n",
    "    # Insert a newline after each complete dialogue entry\n",
    "    formatted_text = re.sub(r\"(#Person\\d+#: [^#]+)\", r\"\\1\\n\", text).strip()\n",
    "\n",
    "    return formatted_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 291,
     "status": "ok",
     "timestamp": 1739715776829,
     "user": {
      "displayName": "Andrea Crobu",
      "userId": "01110819918100512364"
     },
     "user_tz": -60
    },
    "id": "j0m5ntRtPzP-"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# select three random dialogues from the list\n",
    "indeces = random.sample(range(len(model_predictions)), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y7EiSrUu3cXT"
   },
   "source": [
    "### Human evaluation of the predicted summary. Base model vs fine-tuned mdoel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ir_CblCFJVMk"
   },
   "source": [
    "#### **We can notice that the output of the fine-tuned model is mode extensive and is closer to the original summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1368,
     "status": "ok",
     "timestamp": 1739715779033,
     "user": {
      "displayName": "Andrea Crobu",
      "userId": "01110819918100512364"
     },
     "user_tz": -60
    },
    "id": "UpaJb8n9PzNH",
    "outputId": "39a16b9a-0135-4106-fb05-43035c29236e"
   },
   "outputs": [],
   "source": [
    "\n",
    "# print the original dialogue and compare the grounf truth to the base model prediction and the fine-tuned model\n",
    "for idx in indeces:\n",
    "  print()\n",
    "  print('-'*150)\n",
    "  print('-'*150)\n",
    "  print()\n",
    "  print(f'Dialogue {idx}')\n",
    "  print()\n",
    "\n",
    "  original_dialogue = tokenizer.decode(dataset['test']['input_ids'][idx], skip_special_tokens=True)\n",
    "\n",
    "  print('Original dialogue:\\n')\n",
    "  print(format_dialogue(original_dialogue))\n",
    "\n",
    "  print()\n",
    "  print('-'*150)\n",
    "  print()\n",
    "\n",
    "  print('Ground truth summary:\\n')\n",
    "  print(model_predictions['ground_truth'][idx])\n",
    "\n",
    "  print()\n",
    "  print('-'*150)\n",
    "  print()\n",
    "\n",
    "  print('Base model summary:\\n')\n",
    "  print(model_predictions['base_model'][idx])\n",
    "\n",
    "  print()\n",
    "  print('-'*150)\n",
    "  print()\n",
    "\n",
    "  print('Fine-tuned model summary:\\n')\n",
    "  print(model_predictions['fine_tuned_model'][idx])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
