{"cells":[{"cell_type":"markdown","metadata":{"id":"5HCIg39APoFk"},"source":["# Phase 3: Model Evaluation\n","\n","In the final phase, we will evaluate the performance of the fine-tuned Flan-T5 model. The evaluation involves comparing the summarization outputs of the fine-tuned model against the non-fine-tuned (base) model and the ground truth summaries. We will use ROUGE scores and qualitative assessments.\n","\n","## Steps:\n","\n","1. **Compute Evaluation Metrics:**\n","    - **ROUGE Scores:** Measure ROUGE-1, ROUGE-2, and ROUGE-L to assess the quality of the generated summaries.\n","\n","2. **Generate and Compare Summaries:**\n","    - **Qualitative Comparison:** Review summaries generated by the fine-tuned model, the base model, and compare them to the ground truth. Assess the quality and relevance of the generated summaries through manual inspection.\n","    - Identify strengths and weaknesses to guide further improvements.\n","\n","This final phase validates the effectiveness of the fine-tuning process and highlights the improvements achieved in the model's summarization capabilities.\n","\n","---"]},{"cell_type":"code","source":["# %pip install --upgrade pip\n","# %pip install --disable-pip-version-check \\\n","#     torch==1.13.1 \\\n","#     torchdata==0.5.1 --quiet\n","\n","!pip install \\\n","    transformers==4.27.2 \\\n","    datasets==2.11.0 \\\n","    evaluate==0.4.0 \\\n","    rouge_score==0.1.2 \\\n","    loralib==0.1.1 \\\n","    peft==0.3.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"bfniAeF7NinY","executionInfo":{"status":"ok","timestamp":1739714266408,"user_tz":-60,"elapsed":6013,"user":{"displayName":"Andrea Crobu","userId":"01110819918100512364"}},"outputId":"1a0b9400-a356-4e44-9271-67a1a5e29ee3"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers==4.27.2 in /usr/local/lib/python3.11/dist-packages (4.27.2)\n","Requirement already satisfied: datasets==2.11.0 in /usr/local/lib/python3.11/dist-packages (2.11.0)\n","Requirement already satisfied: evaluate==0.4.0 in /usr/local/lib/python3.11/dist-packages (0.4.0)\n","Requirement already satisfied: rouge_score==0.1.2 in /usr/local/lib/python3.11/dist-packages (0.1.2)\n","Requirement already satisfied: loralib==0.1.1 in /usr/local/lib/python3.11/dist-packages (0.1.1)\n","Requirement already satisfied: peft==0.3.0 in /usr/local/lib/python3.11/dist-packages (0.3.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.27.2) (3.17.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.27.2) (0.28.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.27.2) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.27.2) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.27.2) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.27.2) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.27.2) (2.32.3)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.27.2) (0.13.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.27.2) (4.67.1)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.11.0) (17.0.0)\n","Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.11.0) (0.3.6)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets==2.11.0) (2.2.2)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets==2.11.0) (3.5.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets==2.11.0) (0.70.14)\n","Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets==2.11.0) (2024.10.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets==2.11.0) (3.11.12)\n","Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.11/dist-packages (from datasets==2.11.0) (0.18.0)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score==0.1.2) (1.4.0)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score==0.1.2) (3.9.1)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score==0.1.2) (1.17.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft==0.3.0) (5.9.5)\n","Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.3.0) (2.5.1+cu124)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (from peft==0.3.0) (1.3.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.11.0) (2.4.6)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.11.0) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.11.0) (25.1.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.11.0) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.11.0) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.11.0) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.11.0) (1.18.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.27.2) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.27.2) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.27.2) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.27.2) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.27.2) (2025.1.31)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.3.0) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.3.0) (3.1.5)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.3.0) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.3.0) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.3.0) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.3.0) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.3.0) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.3.0) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.3.0) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.3.0) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.3.0) (12.3.1.170)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.3.0) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.3.0) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.3.0) (12.4.127)\n","Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.3.0) (3.1.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.3.0) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft==0.3.0) (1.3.0)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate->peft==0.3.0) (0.5.2)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score==0.1.2) (8.1.8)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score==0.1.2) (1.4.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.11.0) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.11.0) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.11.0) (2025.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft==0.3.0) (3.0.2)\n"]}]},{"cell_type":"code","source":["from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig\n","from peft import PeftModel\n","\n","import torch\n","import pandas as pd\n","import numpy as np"],"metadata":{"id":"CCo1ZsvEPsYh","executionInfo":{"status":"ok","timestamp":1739714277534,"user_tz":-60,"elapsed":8569,"user":{"displayName":"Andrea Crobu","userId":"01110819918100512364"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"Ejj3L9ZDP2Cn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739714279343,"user_tz":-60,"elapsed":1811,"user":{"displayName":"Andrea Crobu","userId":"01110819918100512364"}},"outputId":"7b0de5dc-a826-4634-b5df-c104f4d171ca"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import sys\n","sys.path.append('/content/drive/My Drive/Git_Portfolio')"],"metadata":{"id":"KpnMOKLyP-mV","executionInfo":{"status":"ok","timestamp":1739714284736,"user_tz":-60,"elapsed":536,"user":{"displayName":"Andrea Crobu","userId":"01110819918100512364"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device"],"metadata":{"id":"cIBncdt5P2HQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739714286038,"user_tz":-60,"elapsed":5,"user":{"displayName":"Andrea Crobu","userId":"01110819918100512364"}},"outputId":"cda98280-3f86-40e3-dbd2-4789d2492222"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["### Load the original model and the fine-tuned model (with LoRA)"],"metadata":{"id":"R4-x8NejPqpj"}},{"cell_type":"code","source":["model_path=\"/content/drive/My Drive/Git_Portfolio/results_training/peft-dialogue-summary-checkpoint-local_base\"\n","\n","model_name='google/flan-t5-base'\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","base_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n","\n","peft_model = PeftModel.from_pretrained(base_model, model_path, is_trainable=False)\n","peft_model = peft_model.to(device)\n","peft_model.eval()"],"metadata":{"id":"utmmOGtqP-kY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739714297085,"user_tz":-60,"elapsed":8722,"user":{"displayName":"Andrea Crobu","userId":"01110819918100512364"}},"outputId":"cfbc18b0-da68-4fa7-d900-5eca9a1e0080"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/peft/peft_model.py:372: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  adapters_weights = torch.load(\n"]},{"output_type":"execute_result","data":{"text/plain":["PeftModelForSeq2SeqLM(\n","  (base_model): LoraModel(\n","    (model): T5ForConditionalGeneration(\n","      (shared): Embedding(32128, 768)\n","      (encoder): T5Stack(\n","        (embed_tokens): Embedding(32128, 768)\n","        (block): ModuleList(\n","          (0): T5Block(\n","            (layer): ModuleList(\n","              (0): T5LayerSelfAttention(\n","                (SelfAttention): T5Attention(\n","                  (q): Linear(\n","                    in_features=768, out_features=768, bias=False\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.1, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=768, out_features=24, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=24, out_features=768, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                  )\n","                  (k): Linear(in_features=768, out_features=768, bias=False)\n","                  (v): Linear(\n","                    in_features=768, out_features=768, bias=False\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.1, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=768, out_features=24, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=24, out_features=768, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                  )\n","                  (o): Linear(in_features=768, out_features=768, bias=False)\n","                  (relative_attention_bias): Embedding(32, 12)\n","                )\n","                (layer_norm): T5LayerNorm()\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (1): T5LayerFF(\n","                (DenseReluDense): T5DenseGatedActDense(\n","                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n","                  (dropout): Dropout(p=0.1, inplace=False)\n","                  (act): NewGELUActivation()\n","                )\n","                (layer_norm): T5LayerNorm()\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","          )\n","          (1-11): 11 x T5Block(\n","            (layer): ModuleList(\n","              (0): T5LayerSelfAttention(\n","                (SelfAttention): T5Attention(\n","                  (q): Linear(\n","                    in_features=768, out_features=768, bias=False\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.1, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=768, out_features=24, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=24, out_features=768, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                  )\n","                  (k): Linear(in_features=768, out_features=768, bias=False)\n","                  (v): Linear(\n","                    in_features=768, out_features=768, bias=False\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.1, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=768, out_features=24, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=24, out_features=768, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                  )\n","                  (o): Linear(in_features=768, out_features=768, bias=False)\n","                )\n","                (layer_norm): T5LayerNorm()\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (1): T5LayerFF(\n","                (DenseReluDense): T5DenseGatedActDense(\n","                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n","                  (dropout): Dropout(p=0.1, inplace=False)\n","                  (act): NewGELUActivation()\n","                )\n","                (layer_norm): T5LayerNorm()\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","          )\n","        )\n","        (final_layer_norm): T5LayerNorm()\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (decoder): T5Stack(\n","        (embed_tokens): Embedding(32128, 768)\n","        (block): ModuleList(\n","          (0): T5Block(\n","            (layer): ModuleList(\n","              (0): T5LayerSelfAttention(\n","                (SelfAttention): T5Attention(\n","                  (q): Linear(\n","                    in_features=768, out_features=768, bias=False\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.1, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=768, out_features=24, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=24, out_features=768, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                  )\n","                  (k): Linear(in_features=768, out_features=768, bias=False)\n","                  (v): Linear(\n","                    in_features=768, out_features=768, bias=False\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.1, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=768, out_features=24, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=24, out_features=768, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                  )\n","                  (o): Linear(in_features=768, out_features=768, bias=False)\n","                  (relative_attention_bias): Embedding(32, 12)\n","                )\n","                (layer_norm): T5LayerNorm()\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (1): T5LayerCrossAttention(\n","                (EncDecAttention): T5Attention(\n","                  (q): Linear(\n","                    in_features=768, out_features=768, bias=False\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.1, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=768, out_features=24, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=24, out_features=768, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                  )\n","                  (k): Linear(in_features=768, out_features=768, bias=False)\n","                  (v): Linear(\n","                    in_features=768, out_features=768, bias=False\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.1, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=768, out_features=24, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=24, out_features=768, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                  )\n","                  (o): Linear(in_features=768, out_features=768, bias=False)\n","                )\n","                (layer_norm): T5LayerNorm()\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (2): T5LayerFF(\n","                (DenseReluDense): T5DenseGatedActDense(\n","                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n","                  (dropout): Dropout(p=0.1, inplace=False)\n","                  (act): NewGELUActivation()\n","                )\n","                (layer_norm): T5LayerNorm()\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","          )\n","          (1-11): 11 x T5Block(\n","            (layer): ModuleList(\n","              (0): T5LayerSelfAttention(\n","                (SelfAttention): T5Attention(\n","                  (q): Linear(\n","                    in_features=768, out_features=768, bias=False\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.1, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=768, out_features=24, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=24, out_features=768, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                  )\n","                  (k): Linear(in_features=768, out_features=768, bias=False)\n","                  (v): Linear(\n","                    in_features=768, out_features=768, bias=False\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.1, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=768, out_features=24, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=24, out_features=768, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                  )\n","                  (o): Linear(in_features=768, out_features=768, bias=False)\n","                )\n","                (layer_norm): T5LayerNorm()\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (1): T5LayerCrossAttention(\n","                (EncDecAttention): T5Attention(\n","                  (q): Linear(\n","                    in_features=768, out_features=768, bias=False\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.1, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=768, out_features=24, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=24, out_features=768, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                  )\n","                  (k): Linear(in_features=768, out_features=768, bias=False)\n","                  (v): Linear(\n","                    in_features=768, out_features=768, bias=False\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.1, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=768, out_features=24, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=24, out_features=768, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                  )\n","                  (o): Linear(in_features=768, out_features=768, bias=False)\n","                )\n","                (layer_norm): T5LayerNorm()\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (2): T5LayerFF(\n","                (DenseReluDense): T5DenseGatedActDense(\n","                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n","                  (dropout): Dropout(p=0.1, inplace=False)\n","                  (act): NewGELUActivation()\n","                )\n","                (layer_norm): T5LayerNorm()\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","          )\n","        )\n","        (final_layer_norm): T5LayerNorm()\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n","    )\n","  )\n",")"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["# reload the base model\n","base_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n","base_model = base_model.to(device)\n","base_model.eval()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FR7yIxBYXSgh","executionInfo":{"status":"ok","timestamp":1739714300233,"user_tz":-60,"elapsed":3150,"user":{"displayName":"Andrea Crobu","userId":"01110819918100512364"}},"outputId":"dadcd8a9-637f-4d07-f090-eef562205e78"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["T5ForConditionalGeneration(\n","  (shared): Embedding(32128, 768)\n","  (encoder): T5Stack(\n","    (embed_tokens): Embedding(32128, 768)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","              (relative_attention_bias): Embedding(32, 12)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1-11): 11 x T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (decoder): T5Stack(\n","    (embed_tokens): Embedding(32128, 768)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","              (relative_attention_bias): Embedding(32, 12)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1-11): 11 x T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",")"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"aelzL-ZpR1oZ"}},{"cell_type":"markdown","source":["### Run the originl model and the fine-tuned model on the test set and save the predictions"],"metadata":{"id":"z8Xuru2IR39e"}},{"cell_type":"code","source":["import pickle\n","\n","# Load the dataset\n","with open('/content/drive/My Drive/Git_Portfolio/dataset.pkl', 'rb') as file:\n","    dataset = pickle.load(file)"],"metadata":{"id":"Ybc1a86GP-h6","executionInfo":{"status":"ok","timestamp":1739714304920,"user_tz":-60,"elapsed":867,"user":{"displayName":"Andrea Crobu","userId":"01110819918100512364"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm\n","model_predictions = pd.DataFrame([],columns=['ground_truth', 'base_model', 'fine_tuned_model'])\n","\n","# iterate ove the test set\n","for i in tqdm(range(len(dataset['test']))):\n","  input_ids = dataset['test']['input_ids'][i]\n","  labels = dataset['test']['labels'][i]\n","\n","  # save the ground truth summary\n","  model_predictions.loc[i,'ground_truth'] = tokenizer.decode(labels, skip_special_tokens=True)\n","\n","  # use the base model to predict the summary\n","  outputs_base_model = base_model.generate(\n","        input_ids = torch.tensor(input_ids).unsqueeze(0).to(device),\n","        generation_config=GenerationConfig(max_new_tokens=200)\n","  )\n","\n","  # decode the output and put it in the dataframe\n","  model_predictions.loc[i,'base_model'] = tokenizer.decode(outputs_base_model[0], skip_special_tokens=True)\n","\n","  # use the fine tuned model to predict the summary\n","  outputs_peft_model = peft_model.generate(\n","        input_ids = torch.tensor(input_ids).unsqueeze(0).to(device),\n","        generation_config=GenerationConfig(max_new_tokens=200)\n","  )\n","\n","  # decode the output and put it in the dataframe\n","  model_predictions.loc[i,'fine_tuned_model'] = tokenizer.decode(outputs_peft_model[0], skip_special_tokens=True)"],"metadata":{"id":"PoYXwMDEX57r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# save the predictions on the Test Set\n","model_predictions.to_csv('/content/drive/My Drive/Git_Portfolio/model_predictions.csv', index=False)"],"metadata":{"id":"byKx0Kv02xWX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Compute the metric ROUGE"],"metadata":{"id":"EpKSY0IvRT28"}},{"cell_type":"code","source":["# load the predictions of the test set\n","model_predictions = pd.read_csv('/content/drive/My Drive/Git_Portfolio/model_predictions.csv')"],"metadata":{"id":"dVae0RI5CGcD","executionInfo":{"status":"ok","timestamp":1739714310506,"user_tz":-60,"elapsed":518,"user":{"displayName":"Andrea Crobu","userId":"01110819918100512364"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["import evaluate"],"metadata":{"id":"M4QmOvhbB82s","executionInfo":{"status":"ok","timestamp":1739714332168,"user_tz":-60,"elapsed":269,"user":{"displayName":"Andrea Crobu","userId":"01110819918100512364"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["rouge = evaluate.load('rouge')"],"metadata":{"id":"rFNKjXwrRXu9","colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["7fb6555be25f4d248696492d94a76991","de3dd22c64574a28913add706bcf43ff","cfda7a20ca3f44b9abb2f69072e65b2c","0d5e775f30a4499ca6c1a33d49e590c2","52d7182a3c194476b90fa7f0468ea1cc","d4ac0f045e174362b1e5822bf10bddb3","7a624b5d438c4168bc3559934875e996","44669087829243e19c428157abc7dd58","cdf842a7fecd4be3953acf4321735144","cd689a2f359e4e8095bd7eb0d77d0291","27dcd16290fc451fbfa469b0a1057227"]},"executionInfo":{"status":"ok","timestamp":1739714345212,"user_tz":-60,"elapsed":5034,"user":{"displayName":"Andrea Crobu","userId":"01110819918100512364"}},"outputId":"d1266a71-e3ea-4dd3-c3ee-81b48c6c61b9"},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7fb6555be25f4d248696492d94a76991"}},"metadata":{}}]},{"cell_type":"code","source":["# compute the score for the base model and for the fine tuned model\n","\n","base_model_results = rouge.compute(\n","    predictions=model_predictions['base_model'].to_list(),\n","    references=model_predictions['ground_truth'].to_list(),\n","    use_aggregator=True,\n","    use_stemmer=True,\n",")\n","\n","peft_model_results = rouge.compute(\n","    predictions=model_predictions['fine_tuned_model'].to_list(),\n","    references=model_predictions['ground_truth'].to_list(),\n","    use_aggregator=True,\n","    use_stemmer=True,\n",")"],"metadata":{"id":"T_0rnmPBP-fW","executionInfo":{"status":"ok","timestamp":1739714624550,"user_tz":-60,"elapsed":9538,"user":{"displayName":"Andrea Crobu","userId":"01110819918100512364"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["print('Base Model scores:')\n","for score in base_model_results:\n","  print(f'{score}: {base_model_results[score]}')\n","\n","print('\\n --------------- \\n')\n","\n","print('Fine-tuned Model scores:')\n","for score in peft_model_results:\n","  print(f'{score}: {peft_model_results[score]}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fong2eiCEd8K","executionInfo":{"status":"ok","timestamp":1739714694322,"user_tz":-60,"elapsed":546,"user":{"displayName":"Andrea Crobu","userId":"01110819918100512364"}},"outputId":"5870cb72-c36c-41ed-ca49-7cb29ef628ab"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Base Model scores:\n","rouge1: 0.30828355875756297\n","rouge2: 0.11217798863665049\n","rougeL: 0.2579375123988149\n","rougeLsum: 0.25795189499184956\n","\n"," --------------- \n","\n","Fine-tuned Model scores:\n","rouge1: 0.44021421624662826\n","rouge2: 0.17529958772705015\n","rougeL: 0.3560808212586427\n","rougeLsum: 0.35632161056594747\n"]}]},{"cell_type":"markdown","source":["#### **We can notic how all the rouge scores improved with fine-tuning!**"],"metadata":{"id":"MkAo2SbPFaZO"}},{"cell_type":"markdown","source":["---"],"metadata":{"id":"Tg6yIMF1Psk3"}},{"cell_type":"code","source":["import random"],"metadata":{"id":"peF3sM-jPzSc","executionInfo":{"status":"ok","timestamp":1739714897639,"user_tz":-60,"elapsed":511,"user":{"displayName":"Andrea Crobu","userId":"01110819918100512364"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","execution_count":40,"metadata":{"id":"dbiwqNZ_PoFm","executionInfo":{"status":"ok","timestamp":1739715213856,"user_tz":-60,"elapsed":577,"user":{"displayName":"Andrea Crobu","userId":"01110819918100512364"}}},"outputs":[],"source":["import re\n","\n","def format_dialogue(text):\n","    # Remove the intro and summary prompt\n","    text = re.sub(r\"^Here is a dialogue:\\s*\", \"\", text)  # Remove starting phrase\n","    text = re.sub(r\"\\s*Write a short summary!$\", \"\", text)  # Remove ending phrase\n","\n","    # Insert a newline after each complete dialogue entry\n","    formatted_text = re.sub(r\"(#Person\\d+#: [^#]+)\", r\"\\1\\n\", text).strip()\n","\n","    return formatted_text\n","\n"]},{"cell_type":"code","source":["# select three random dialogues from the list\n","indeces = random.sample(range(len(model_predictions)), 3)"],"metadata":{"id":"j0m5ntRtPzP-","executionInfo":{"status":"ok","timestamp":1739715776829,"user_tz":-60,"elapsed":291,"user":{"displayName":"Andrea Crobu","userId":"01110819918100512364"}}},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":["### Human evaluation of the predicted summary. Base model vs fine-tuned mdoel"],"metadata":{"id":"y7EiSrUu3cXT"}},{"cell_type":"markdown","source":["#### **We can notice that the output of the fine-tuned model is mode extensive and is closer to the original summary**"],"metadata":{"id":"Ir_CblCFJVMk"}},{"cell_type":"code","source":["\n","# print the original dialogue and compare the grounf truth to the base model prediction and the fine-tuned model\n","for idx in indeces:\n","  print()\n","  print('-'*150)\n","  print('-'*150)\n","  print()\n","  print(f'Dialogue {idx}')\n","  print()\n","\n","  original_dialogue = tokenizer.decode(dataset['test']['input_ids'][idx], skip_special_tokens=True)\n","\n","  print('Original dialogue:\\n')\n","  print(format_dialogue(original_dialogue))\n","\n","  print()\n","  print('-'*150)\n","  print()\n","\n","  print('Ground truth summary:\\n')\n","  print(model_predictions['ground_truth'][idx])\n","\n","  print()\n","  print('-'*150)\n","  print()\n","\n","  print('Base model summary:\\n')\n","  print(model_predictions['base_model'][idx])\n","\n","  print()\n","  print('-'*150)\n","  print()\n","\n","  print('Fine-tuned model summary:\\n')\n","  print(model_predictions['fine_tuned_model'][idx])"],"metadata":{"id":"UpaJb8n9PzNH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739715779033,"user_tz":-60,"elapsed":1368,"user":{"displayName":"Andrea Crobu","userId":"01110819918100512364"}},"outputId":"39a16b9a-0135-4106-fb05-43035c29236e"},"execution_count":59,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","\n","Dialogue 574\n","\n","Original dialogue:\n","\n","#Person1#: Hi, I was wondering if I could get my test results from the other day. \n","#Person2#: Yes, I would like to schedule an appointment for you to come in and talk with me. \n","#Person1#: Is something wrong with me? \n","#Person2#: No, sometimes the test results aren't clear and we need to do more to get a clearer picture. \n","#Person1#: Can we talk about it now? \n","#Person2#: I would if I knew anything for sure, but I want to take a second look. \n","#Person1#: When can I come and see you? \n","#Person2#: You can come in this afternoon. If you would feel better, bring your husband with you. \n","#Person1#: Now I know that something bad is up! \n","#Person2#: Just relax. We will talk about it this afternoon.\n","\n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","\n","Ground truth summary:\n","\n","#Person2# schedules an appointment with #Person1# and #Person1#'s husband. #Person1# is nervous to know the test results.\n","\n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","\n","Base model summary:\n","\n","Person1 wants to schedule an appointment for Person2 to come in and talk about the test results from the other day.\n","\n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","\n","Fine-tuned model summary:\n","\n","#Person1# wants to get the test results from the other day. #Person2# tells #Person1# they need to do more to get a clearer picture. #Person1# will come and talk about it this afternoon.\n","\n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","\n","Dialogue 1063\n","\n","Original dialogue:\n","\n","#Person1#: Say, Jim, how about going for a few beers after dinner? \n","#Person2#: You know that is tempting but is really not good for our fitness. \n","#Person1#: What do you mean? It will help us to relax. \n","#Person2#: Do you really think so? I don't. It will just make us fat and act silly. Remember last time? \n","#Person1#: I guess you are right. But what shall we do? I don't feel like sitting at home. \n","#Person2#: I suggest a walk over to the gym where we can play singsong and meet some of our friends. \n","#Person1#: That's a good idea. I hear Mary and Sally often go there to play pingpong. Perhaps we can make a foursome with them. \n","#Person2#: Sounds great to me! If they are willing, we could ask them to go dancing with us. That is excellent exercise and fun, too. \n","#Person1#: Good. Let's go now. \n","#Person2#: All right.\n","\n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","\n","Ground truth summary:\n","\n","#Person1# feels bored at home and asks Jim go for a beer. Jim refuses and suggests going to the gym and meeting friends.\n","\n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","\n","Base model summary:\n","\n","Jim and Mary are going to the gym to play pingpong and ask Mary and Sally to go dancing.\n","\n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","\n","Fine-tuned model summary:\n","\n","Jim suggests going for a few beers after dinner but #Person1# thinks it's not good for their fitness. They decide to go to the gym and ask Mary and Sally to go dancing.\n","\n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","\n","Dialogue 522\n","\n","Original dialogue:\n","\n","#Person1#: Welcome, what would you like to order? \n","#Person2#: I would like to get a double cheeseburger. \n","#Person1#: Would you like everything on it? \n","#Person2#: I would like everything on it, thank you. \n","#Person1#: Do you want any fries? \n","#Person2#: Let me get some large curly fries. \n","#Person1#: Can I get you anything to drink? \n","#Person2#: Sure, how about a medium Pepsi? \n","#Person1#: Is that everything? \n","#Person2#: That'll be all. Thanks. \n","#Person1#: You're welcome, and your total is $ 5. 48. \n","#Person2#: Thank you. Here you go.\n","\n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","\n","Ground truth summary:\n","\n","#Person2# orders a $ 5. 48 meal including cheeseburger, fries, and Pepsi.\n","\n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","\n","Base model summary:\n","\n","Person1 wants to order a double cheeseburger.\n","\n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","\n","Fine-tuned model summary:\n","\n","#Person2# orders a double cheeseburger, fries, and Pepsi.\n"]}]}],"metadata":{"language_info":{"name":"python"},"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"7fb6555be25f4d248696492d94a76991":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_de3dd22c64574a28913add706bcf43ff","IPY_MODEL_cfda7a20ca3f44b9abb2f69072e65b2c","IPY_MODEL_0d5e775f30a4499ca6c1a33d49e590c2"],"layout":"IPY_MODEL_52d7182a3c194476b90fa7f0468ea1cc"}},"de3dd22c64574a28913add706bcf43ff":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d4ac0f045e174362b1e5822bf10bddb3","placeholder":"​","style":"IPY_MODEL_7a624b5d438c4168bc3559934875e996","value":"Downloading builder script: 100%"}},"cfda7a20ca3f44b9abb2f69072e65b2c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_44669087829243e19c428157abc7dd58","max":6270,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cdf842a7fecd4be3953acf4321735144","value":6270}},"0d5e775f30a4499ca6c1a33d49e590c2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cd689a2f359e4e8095bd7eb0d77d0291","placeholder":"​","style":"IPY_MODEL_27dcd16290fc451fbfa469b0a1057227","value":" 6.27k/6.27k [00:00&lt;00:00, 193kB/s]"}},"52d7182a3c194476b90fa7f0468ea1cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d4ac0f045e174362b1e5822bf10bddb3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7a624b5d438c4168bc3559934875e996":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"44669087829243e19c428157abc7dd58":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cdf842a7fecd4be3953acf4321735144":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cd689a2f359e4e8095bd7eb0d77d0291":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"27dcd16290fc451fbfa469b0a1057227":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}